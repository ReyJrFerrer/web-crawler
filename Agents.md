# Agent Mode PRD: Scalable Web Crawler System - Phase 2
**3-Day Sprint Implementation Guide**

## 1. Overview

### Purpose
Iterate and improve the MVP of the distributed web crawler system, fulfilling *all* advanced requirements specified in the Project Design Report (PDR). This transitions the crawler from a basic pipeline to a massively scalable, fault-tolerant, and polite distributed architecture.

### Sprint Goal
Deliver a production-hardened crawler capable of rendering Single Page Applications (SPAs), preventing spider traps, deduplicating content via fingerprints, compressing stored data in object storage, graceful crash recovery, dynamic rate limiting, and egress proxy rotation.

### Methodology
Implement a Test-Driven Development Methodology. Write tests before or while implementing the function. Do not determine that the task is done if there are lint issues or incomplete/failed tests.

After every feature implemented, place them on the existing changelog

### Current Tools
Bun, Redis (Bull), MongoDB, Cheerio, Axios/Fetch

---

## 2. System Architecture

### Core Components (Priority Order)

#### Day 1: Advanced Processing & Scale Foundation
1. **Renderer Agent** (Headless Browser)
   - Tech: Puppeteer or Playwright
   - Features: Execute JavaScript and render dynamic Single Page Applications (SPAs) before parsing.
   - Priority: P0 (Critical)

2. **Spider Trap Protector**
   - Tech: URL depth limits and Path-repetition detection algorithm
   - Features: Prevent infinite loops dynamically generated by servers (e.g., calendar/2026/01/01...).
   - Priority: P0 (Critical)

3. **Domain Restriction Filter**
   - Tech: Hostname parsing and matching
   - Features: Prevent the crawler from escaping the original target domain by filtering out external links (e.g., YouTube, Facebook) during the parsing phase.
   - Priority: P0 (Critical)

4. **Content Duplicate Eliminator**
   - Tech: Simhash or MinHash algorithm
   - Features: Fingerprint page content to detect and drop near-duplicates or mirror sites.
   - Priority: P1 (High)

5. **Distributed URL Frontier Enhancements**
   - Tech: Redis (Bull MQ)
   - Features: Implement domain-based partitioning (hash routing) so all URLs for a single domain route to the same Fetcher node.
   - Priority: P0 (Critical)

6. **Integrate Dashboard**
   - Features: Integrate backend to dashboard for UI/UX convenience
   - Priority: Mid

#### Day 2: Optimization, Fault Tolerance & Politeness
7. **Storage Optimizer & Object Storage Adapter**
   - Tech: Brotli compression (`zlib`), AWS S3 -> Changed to DigitalOcean
   - Features: Compress raw HTML prior to storage; migrate raw HTML storage to S3/Digital Ocean; implement strict 30-day data retention policies (TTL).
   - Priority: P0 (Critical)

8. **Dynamic Politeness & Egress Manager**
   - Tech: Proxy rotation libraries, dynamic backoff algorithms
   - Features: Adjust crawl delay dynamically based on server response degradation; maintain a diverse pool of egress IP addresses/proxies to mitigate WAF blocking. Bypass/ mitigate error 429 and other related issues.
   - Priority: P0 (Critical)

9. **Fault Tolerance Manager**
   - Tech: Redis (Bull MQ) Un-ack mechanisms
   - Features: Crash recovery (unacknowledged URLs returned to the queue if a Fetcher dies); Dead-letter queues for URLs that repeatedly timeout. Mitigate error 403 and other related issues
   - Priority: P0 (Critical)

10. **Indexer Integration / Extensibility Layer**
   - Tech: Pluggable architecture
   - Features: Structure the parsing layer to easily plug in Elasticsearch indexing or NLP entities extraction modules without rewriting the core loop.
   - Priority: P2 (Nice-to-have)


#### Day 3: Infrastructure & Scaling Setup
**Goal:** Implement Horizontal Scaling, Cloud Storage, and CLI-based Cluster Management.

**Tasks:** 

11. **Integrate DigitalOcean Spaces (Object Storage)**
   - Features: Finalize S3-compatible adapter for persisting raw HTML.
   - Priority: P1 (High)
   - Additional - 
12. **CLI-based Fetcher Scaling (Kubernetes)**
   - Tech: Node/Bun `@kubernetes/client-node`
   - Features: Build a CLI service to dynamically scale fetcher pods up/down via Kubernetes API. Prototype locally on Docker Desktop, deploy to DigitalOcean Kubernetes (DOKS).
   - Priority: P1 (High)
---

## 3. Agent Behaviors

### Renderer Agent (New)
**Autonomy Level:** Autonomous
- **Input:** URLs identified as SPAs or lacking standard static HTML content.
- **Actions:**
  1. Launch headless browser context.
  2. Navigate to URL and wait for network idle/JS execution.
  3. Extract fully rendered HTML.
  4. Pass rendered HTML to Storage Subsystem and Parser.
- **Error Handling:** Timeout after 30s, log failure, fallback to raw HTML.

### Fetcher Agent (Updated)
**Autonomy Level:** Semi-autonomous
- **New Actions:**
  - Enforce domain-based hash routing (receive only URLs for assigned domains).
  - Route through egress proxies to avoid IP Blocking.
  - Measure response times to adjust dynamic crawl delay.
  - Route standard HTML to Storage/Parser, but route suspected SPAs to Renderer Agent.
- **Error Handling:** Un-ack message on sudden crash so it returns to the Frontier. Route to Dead-Letter Queue after 3 failures.

### Parser Agent (Updated)
**Autonomy Level:** Autonomous
- **New Actions:**
  - Apply Spider Trap detection (URL depth tracking, path repetition validation).
  - Apply Domain Restriction Filtering (extract hostname from seed URL, discard external outbound links).
  - Pluggable extraction: easily hook in image extraction or text indexing modules.

### Duplicate Eliminator Agent (Updated)
**Autonomy Level:** Autonomous
- **New Actions:**
  - In addition to Bloom Filter (URL dedup), compute Simhash for extracted text.
  - Compare Simhash against recently stored content to drop near-duplicates.

---

## 4. Data Flow

```
Seed Injector → URL Frontier (Domain Partitioned)
                      ↓ (Message Un-ack for Crash Recovery)
                Fetcher Agent → DNS Cache
                 ↙         ↘
     (Standard HTML)      (SPA / Dynamic)
           ↓                    ↓
           |               Renderer Agent
           ↘         ↙
        Storage (Compressed Raw HTML → S3/Object Storage)
                      ↓
                 Parser Agent (Spider Trap Check)
                      ↓
  Duplicate Eliminator (URL Bloom Filter + Content Simhash)
                      ↓
  Indexer/Extensibility Layer (e.g., Elasticsearch adapter)
                      ↓
              URL Frontier (loop)
                      ↓ (On 3x Failure)
              Dead-Letter Queue
```

---

## 5. Technical Specifications

### Tech Stack Additions
- **Runtime:** Bun
- **Renderer:** `puppeteer`
- **Content Dedup:** Simhash libraries (e.g., `simhash-js`)
- **Compression:** Native `zlib` (Brotli/GZIP)
- **Object Storage:** `aws-sdk` (for S3 compatibility)
- **Extensibility:** Pluggable module interfaces
- **Proxy Management:** Standard HTTP/HTTPS proxy agents

### Infrastructure
- **Scaling:** Utilize DigitalOcean Kubernetes (DOKS) for horizontal scaling of Fetcher Deployments.
- **Storage Evolution:** Shift raw HTML payload to S3-compatible object storage; keep MongoDB for metadata/parsed data only.

---

## 6. Sprint Breakdown

### Day 1: Advanced Processing & Scale Foundation (8 hours)
**Goal:** Handle dynamic content, prevent infinite loops, deduplicate content, and structure the queue for scale.

**Tasks:**
1. **Renderer Agent:** Integrate Puppeteer for SPA rendering and routing logic in Fetcher. - 2h
2. **Spider Trap & Domain Protection:** Implement URL depth limits, repetition regex, and Domain Restriction Filtering in Parser. - 1.5h
3. **Content Deduplication:** Implement Simhash fingerprinting for parsed text. - 2h
4. **Queue Partitioning:** Refactor Redis URL Frontier to use domain-based hash routing. - 1.5h
5. **Integration Test:** Crawl a known SPA and verify duplicate content and external links are rejected. - 1h

**Deliverable:** Crawler can read SPAs, avoid infinite paths, drop mirrored content, stay within domain bounds, and partition queues by domain.

### Day 2: Optimization, Fault Tolerance & Politeness (8 hours)
**Goal:** Optimize storage costs, prevent IP bans, ensure crash recovery, and finalize extensibility.

**Tasks:**
1. **Storage Optimizer:** Compress HTML using Brotli/GZIP; build S3 Object Storage adapter; implement 30-day retention cleanup script. - 2h
2. **Dynamic Politeness:** Track response times and dynamically adjust `CRAWL_DELAY`; implement Proxy rotation logic. - 2h
3. **Fault Tolerance:** Implement Redis message un-ack mechanisms for crash recovery and Dead-letter queues for persistent failures. - 1.5h
4. **Extensibility Layer:** Refactor Parser to support pluggable modules (Indexer, Image Extractor). - 1.5h
5. **Load Test:** Verify memory/storage footprint with compression enabled and test crash recovery by killing a Fetcher. - 1h

**Deliverable:** Production-grade crawler with low storage footprint, high resilience, fault recovery, and strict politeness.

### Day 3: Infrastructure & Scaling Setup (8 hours)
**Goal:** Containerize the fetcher, define Kubernetes manifests, and build a CLI service for cluster management.

**Tasks:**
1. **Containerization:** Write `Dockerfile` for the fetcher agent (Bun + Puppeteer). - 1h
2. **Local Kubernetes Manifests:** Draft `ConfigMap`, `Deployment`, and `Service` YAMLs for local testing via Docker Desktop. - 1.5h
3. **CLI Management Service:** Build a Node/Bun CLI tool using `@kubernetes/client-node` to interact with the local cluster and scale fetcher replicas. - 2.5h
4. **Production DOKS Strategy:** Adapt manifests for DigitalOcean Kubernetes and document deployment steps. - 1h
5. **DigitalOcean Spaces:** Finalize integration and test uploading compressed HTML to DigitalOcean Spaces. - 2h

**Deliverable:** A containerized fetcher scaling dynamically on a local Kubernetes cluster via a custom CLI tool, ready for production DOKS deployment, with fully integrated DigitalOcean object storage.

---

## 7. Configuration

### Environment Variables
```bash
# Existing
REDIS_URL=redis://localhost:6379
MONGO_URL=mongodb://localhost:27017/crawler
FETCHER_CONCURRENCY=10
CRAWL_DELAY_MS=2000
USER_AGENT="MyCrawler/2.0 (+http://example.com/bot)"

# New for Phase 2
MAX_DEPTH=5
USE_RENDERER=true
PROXY_LIST_URL=http://internal-proxy-service/list
SIMHASH_THRESHOLD=3
COMPRESSION_ALGO=brotli # or gzip
S3_BUCKET_NAME=crawler-raw-html
S3_REGION=us-east-1
S3_RETENTION_DAYS=30
```

---

## 8. Non-Functional Requirements

### Performance & Storage
- **Storage:** Reduce raw HTML storage footprint by ~70-80% using Brotli/Gzip compression.
- **Renderer Latency:** < 10s per SPA page.
- **Data Gravity:** Strict 30-day TTL enforcement on Raw HTML in object storage.

### Resiliency
- **Spider Traps:** 100% prevention of loops via URL validation.
- **WAF Mitigation:** Automatic backoff on HTTP 429/503 and IP rotation.
- **Crash Recovery:** 0 lost URLs if a Fetcher node unexpectedly dies.

---

## 9. Known Limitations (Phase 2 Scope)

### Out of Scope (P3)
- Real-time Elasticsearch cluster deployment (we only build the pluggable adapter for now).
- Complex NLP entity recognition.

---

## 10. Testing Strategy

### Unit Tests
- Spider Trap regex matching and path validation.
- Domain restriction filtering algorithm.
- Simhash generation and distance comparison accuracy.
- GZIP/Brotli compression and decompression integrity.
- Hash routing validation for URL domains.

### Integration Tests
1. **SPA Test:** Fetch a React/Vue SPA; verify Rendered HTML contains target text.
2. **Trap Test:** Feed a calendar spider trap; verify it stops at `MAX_DEPTH`.
3. **Dynamic Politeness Test:** Mock a slow server; verify the Fetcher's delay automatically increases.
4. **Crash Test:** Manually kill a Fetcher mid-download; verify the URL returns to the queue.

---

## 11. Success Metrics

### Sprint Goals
- ✅ Successful rendering of JavaScript-heavy pages.
- ✅ Storage migrated to object storage with compression.
- ✅ Spider traps successfully detected and ignored.
- ✅ Content duplicates rejected via Simhash.
- ✅ Crash recovery confirmed (0 lost messages).
- ✅ Dynamic rate limiting and proxy rotation functional.

---

## 12. Risk Mitigation

| Risk | Mitigation |
|------|------------|
| **Puppeteer Memory Leaks** | Re-use browser contexts, implement strict page timeouts, and restart browser instance every X requests. |
| **Compression CPU Overhead** | Offload compression to worker threads if it blocks the main event loop. |
| **Simhash False Positives** | Tune the Hamming distance threshold through testing on diverse datasets. |
| **Redis Memory Exhaustion** | Use Bull's built-in limits for completed/failed jobs to prevent unbounded memory growth. |



**End of Document**