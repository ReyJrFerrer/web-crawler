Feb 21, 2026
Developer: Reynaldo 
- Advanced Processing & Scale Foundation:
    - Added `puppeteer` dependency for headless browser rendering.
    - Implemented `RendererAgent` to execute JavaScript and render dynamic Single Page Applications (SPAs).
    - Updated `FetcherAgent` with a heuristic to detect suspected SPAs and route them to the `RendererAgent`.
    - Added resource blocking (images, media, fonts, css) in `RendererAgent` to optimize rendering speed.
    - Implemented memory leak protection in `RendererAgent` by automatically restarting the browser context every 50 requests.
    - Updated `config.ts` to include `USE_RENDERER` environment variable.
    - Added `tests/renderer.test.ts` to verify SPA rendering and network idle waiting.
    - Updated orchestrator (`src/index.ts`) to gracefully initialize and close the `RendererAgent`.

-  Deduplicate Content:
    - Added `simhash-js` dependency for content fingerprinting.
    - Updated `DuplicateEliminator` to compute Simhash for extracted text and detect near-duplicates by calculating Hamming distance against recently stored content.
    - Updated `ParserAgent` to correctly extract clean body text, ignoring `script` and `style` tags, for accurate deduplication.
    - Integrated duplicate content checking within `FetcherAgent` to efficiently drop mirror sites.
    - Updated `config.ts` to include `SIMHASH_THRESHOLD` environment variable.
    - Added and updated tests (`tests/eliminator.test.ts`, `tests/parser.test.ts`) to verify duplicate detection and robust text extraction.

- Spider Trap and Domain Restriction Filter:
    - Implemented Spider Trap Protector in `ParserAgent` (URL depth limits & Path-repetition detection algorithm).
    - Implemented Domain Restriction Filter in `ParserAgent` to prevent escaping the original target domain.
    - Updated `tests/parser.test.ts` to verify domain restriction and spider trap handling.

    
- Distributed URL Frontier Enhancements:
    - Updated `Frontier` to implement domain-based hash routing, hashing the domain to assign jobs to specific partitions.
    - Updated `FetcherAgent` to configure itself based on designated partition IDs (`WORKER_PARTITION_IDS`) or fall back to listening to all partitions.
    - Added `QUEUE_PARTITIONS` and `WORKER_PARTITION_IDS` configuration variables to `config.ts`.
    - Added hash routing validation tests in `tests/frontier.test.ts`.

- Dashboard Integration:
    - Built the `crawler-dashboard` Vite React app and configured it to be served by an Express BFF server.
    - Updated `src/frontend/server/index.ts` to properly handle Express 5 `path-to-regexp` catch-all routing for SPA functionality.
    - Added `express` and `cors` to the root project dependencies for the BFF API.
    - Fixed incorrect `config` import paths in `control.ts`, `dataExplorer.ts`, `queueMetrics.ts`, and `errorLogs.ts` inside the `src/frontend/server/` directory.
    - Updated `src/index.ts` to expose the backend-for-frontend on port 4000 alongside the crawler's orchestrator layer.
    - Resolved Vite to BFF proxy `ECONNREFUSED` issues by migrating bindings from `localhost` to `127.0.0.1`.
    - Stripped out all dummy `MOCK` payloads and variables from the Express data providers (`queueMetrics`, `dataExplorer`, `workerHealth`, `errorLogs`) guaranteeing real-time system state mirroring instead of mock fallbacks.


Bugs Found 
- Too Deep Fetching
    - Description: 
    When trying to crawl SRV, it scraped the index file after using the renderer, but after encountering youtube video links it then fetched from the youtube site.

    - Fixes: 
    Implemented Domain Restriction Filter in ParserAgent to prevent the crawler from escaping the original target domain.
    Refined Domain Restriction Filter to allow 1-hop embedded links (like youtube or tiktok) from the seed domain but restrict further out-of-bounds crawling by passing originalDomain through the Frontier Queue.
    
Developer: Jan Dale 

Feb 20, 2026 
Developer: Reynaldo 
- Initialized MVP Development: 
    - Set up project structure with Bun and TypeScript.
    - Implemented `FetcherAgent` for core HTML fetching pipeline.
    - Integrated Redis (`bull`) as the URL Frontier queue.
    - Integrated MongoDB as the Storage Service for raw HTML and parsed metadata.
    - Implemented `ParserAgent` using Cheerio to extract titles and links.
    - Implemented `DuplicateEliminator` using a Bloom Filter for URL deduplication.
    - Added DNS Caching (`cacheable-lookup`) and Rate Limiting politeness controls.
    - Added `robots.txt` parsing to respect site crawling rules.
    - Configured ESLint/Biome for formatting and linting.
    - Setup initial test suites for agents and services.

